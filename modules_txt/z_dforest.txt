''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Copyright (c) 2009, Sergey Bochkanov (ALGLIB project).
'
'>>> SOURCE LICENSE >>>
'This program is free software; you can redistribute it and/or modify
'it under the terms of the GNU General Public License as published by
'the Free Software Foundation (www.fsf.org); either version 2 of the
'License, or (at your option) any later version.
'
'This program is distributed in the hope that it will be useful,
'but WITHOUT ANY WARRANTY; without even the implied warranty of
'MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
'GNU General Public License for more details.
'
'A copy of the GNU General Public License is available at
'http://www.fsf.org/licensing/licenses
'
'>>> END OF LICENSE >>>
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Data types
Public Type DecisionForest
    NVars As Long
    NClasses As Long
    NTrees As Long
    BufSize As Long
    Trees() As Double
End Type
Public Type DFReport
    RelCLSError As Double
    AvgCE As Double
    RMSError As Double
    AvgError As Double
    AvgRelError As Double
    OOBRelClsError As Double
    OOBAvgCE As Double
    OOBRMSError As Double
    OOBAvgError As Double
    OOBAvgRelError As Double
End Type
Public Type DFInternalBuffers
    TreeBuf() As Double
    IdxBuf() As Long
    TmpBufR() As Double
    TmpBufR2() As Double
    TmpBufI() As Long
    ClassIBuf() As Long
    VarPool() As Long
    EVSBin() As Boolean
    EVSSplits() As Double
End Type
'Global constants
Private Const DFVNum As Long = 8#
Private Const InnerNodeWidth As Long = 3#
Private Const LeafNodeWidth As Long = 2#
Private Const DFUseStrongSplits As Long = 1#
Private Const DFUseEVS As Long = 2#
'Routines
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'This subroutine builds random decision forest.
'
'INPUT PARAMETERS:
'    XY          -   training set
'    NPoints     -   training set size, NPoints>=1
'    NVars       -   number of independent variables, NVars>=1
'    NClasses    -   task type:
'                    * NClasses=1 - regression task with one
'                                   dependent variable
'                    * NClasses>1 - classification task with
'                                   NClasses classes.
'    NTrees      -   number of trees in a forest, NTrees>=1.
'                    recommended values: 50-100.
'    R           -   percent of a training set used to build
'                    individual trees. 0<R<=1.
'                    recommended values: 0.1 <= R <= 0.66.
'
'OUTPUT PARAMETERS:
'    Info        -   return code:
'                    * -2, if there is a point with class number
'                          outside of [0..NClasses-1].
'                    * -1, if incorrect parameters was passed
'                          (NPoints<1, NVars<1, NClasses<1, NTrees<1, R<=0
'                          or R>1).
'                    *  1, if task has been solved
'    DF          -   model built
'    Rep         -   training report, contains error on a training set
'                    and out-of-bag estimates of generalization error.
'
'  -- ALGLIB --
'     Copyright 19.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Sub DFBuildRandomDecisionForest(ByRef XY() As Double, _
         ByVal NPoints As Long, _
         ByVal NVars As Long, _
         ByVal NClasses As Long, _
         ByVal NTrees As Long, _
         ByVal R As Double, _
         ByRef Info As Long, _
         ByRef DF As DecisionForest, _
         ByRef Rep As DFReport)
    Dim SampleSize As Long
    If R <= 0# Or R > 1# Then
        Info = -1#
        Exit Sub
    End If
    SampleSize = MaxInt(Round(R * NPoints), 1#)
    Call DFBuildInternal(XY, NPoints, NVars, NClasses, NTrees, SampleSize, MaxInt(NVars \ 2#, 1#), DFUseStrongSplits + DFUseEVS, Info, DF, Rep)
End Sub
Public Sub DFBuildInternal(ByRef XY() As Double, _
         ByVal NPoints As Long, _
         ByVal NVars As Long, _
         ByVal NClasses As Long, _
         ByVal NTrees As Long, _
         ByVal SampleSize As Long, _
         ByVal NFeatures As Long, _
         ByVal Flags As Long, _
         ByRef Info As Long, _
         ByRef DF As DecisionForest, _
         ByRef Rep As DFReport)
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim TmpI As Long
    Dim LastTreeOffs As Long
    Dim Offs As Long
    Dim OOBOffs As Long
    Dim TreeSize As Long
    Dim NVarsInPool As Long
    Dim UseEVS As Boolean
    Dim Bufs As DFInternalBuffers
    Dim PermBuf() As Long
    Dim OOBBuf() As Double
    Dim OOBCntBuf() As Long
    Dim XYS() As Double
    Dim X() As Double
    Dim y() As Double
    Dim OOBCnt As Long
    Dim OOBRelCnt As Long
    Dim V As Double
    Dim VMin As Double
    Dim VMax As Double
    Dim BFlag As Boolean
    Dim i_ As Long
    Dim i1_ As Long
    
    '
    ' Test for inputs
    '
    If NPoints < 1# Or SampleSize < 1# Or SampleSize > NPoints Or NVars < 1# Or NClasses < 1# Or NTrees < 1# Or NFeatures < 1# Then
        Info = -1#
        Exit Sub
    End If
    If NClasses > 1# Then
        For i = 0# To NPoints - 1# Step 1
            If Round(XY(i, NVars)) < 0# Or Round(XY(i, NVars)) >= NClasses Then
                Info = -2#
                Exit Sub
            End If
        Next i
    End If
    Info = 1#
    
    '
    ' Flags
    '
    UseEVS = Flags \ DFUseEVS Mod 2# <> 0#
    
    '
    ' Allocate data, prepare header
    '
    TreeSize = 1# + InnerNodeWidth * (SampleSize - 1#) + LeafNodeWidth * SampleSize
    ReDim PermBuf(0# To NPoints - 1#)
    ReDim Bufs.TreeBuf(0# To TreeSize - 1#)
    ReDim Bufs.IdxBuf(0# To NPoints - 1#)
    ReDim Bufs.TmpBufR(0# To NPoints - 1#)
    ReDim Bufs.TmpBufR2(0# To NPoints - 1#)
    ReDim Bufs.TmpBufI(0# To NPoints - 1#)
    ReDim Bufs.VarPool(0# To NVars - 1#)
    ReDim Bufs.EVSBin(0# To NVars - 1#)
    ReDim Bufs.EVSSplits(0# To NVars - 1#)
    ReDim Bufs.ClassIBuf(0# To 2# * NClasses - 1#)
    ReDim OOBBuf(0# To NClasses * NPoints - 1#)
    ReDim OOBCntBuf(0# To NPoints - 1#)
    ReDim DF.Trees(0# To NTrees * TreeSize - 1#)
    ReDim XYS(0# To SampleSize - 1#, 0# To NVars)
    ReDim X(0# To NVars - 1#)
    ReDim y(0# To NClasses - 1#)
    For i = 0# To NPoints - 1# Step 1
        PermBuf(i) = i
    Next i
    For i = 0# To NPoints * NClasses - 1# Step 1
        OOBBuf(i) = 0#
    Next i
    For i = 0# To NPoints - 1# Step 1
        OOBCntBuf(i) = 0#
    Next i
    
    '
    ' Prepare variable pool and EVS (extended variable selection/splitting) buffers
    ' (whether EVS is turned on or not):
    ' 1. detect binary variables and pre-calculate splits for them
    ' 2. detect variables with non-distinct values and exclude them from pool
    '
    For i = 0# To NVars - 1# Step 1
        Bufs.VarPool(i) = i
    Next i
    NVarsInPool = NVars
    If UseEVS Then
        For j = 0# To NVars - 1# Step 1
            VMin = XY(0#, j)
            VMax = VMin
            For i = 0# To NPoints - 1# Step 1
                V = XY(i, j)
                VMin = MinReal(VMin, V)
                VMax = MaxReal(VMax, V)
            Next i
            If VMin = VMax Then
                
                '
                ' exclude variable from pool
                '
                Bufs.VarPool(j) = Bufs.VarPool(NVarsInPool - 1#)
                Bufs.VarPool(NVarsInPool - 1#) = -1#
                NVarsInPool = NVarsInPool - 1#
                GoTo Cont_6
            End If
            BFlag = False
            For i = 0# To NPoints - 1# Step 1
                V = XY(i, j)
                If V <> VMin And V <> VMax Then
                    BFlag = True
                    Exit For
                End If
            Next i
            If BFlag Then
                
                '
                ' non-binary variable
                '
                Bufs.EVSBin(j) = False
            Else
                
                '
                ' Prepare
                '
                Bufs.EVSBin(j) = True
                Bufs.EVSSplits(j) = 0.5 * (VMin + VMax)
                If Bufs.EVSSplits(j) <= VMin Then
                    Bufs.EVSSplits(j) = VMax
                End If
            End If
Cont_6:
        Next j
    End If
    
    '
    ' RANDOM FOREST FORMAT
    ' W[0]         -   size of array
    ' W[1]         -   version number
    ' W[2]         -   NVars
    ' W[3]         -   NClasses (1 for regression)
    ' W[4]         -   NTrees
    ' W[5]         -   trees offset
    '
    '
    ' TREE FORMAT
    ' W[Offs]      -   size of sub-array
    '     node info:
    ' W[K+0]       -   variable number        (-1 for leaf mode)
    ' W[K+1]       -   threshold              (class/value for leaf node)
    ' W[K+2]       -   ">=" branch index      (absent for leaf node)
    '
    '
    DF.NVars = NVars
    DF.NClasses = NClasses
    DF.NTrees = NTrees
    
    '
    ' Build forest
    '
    Offs = 0#
    For i = 0# To NTrees - 1# Step 1
        
        '
        ' Prepare sample
        '
        For K = 0# To SampleSize - 1# Step 1
            j = K + RandomInteger(NPoints - K)
            TmpI = PermBuf(K)
            PermBuf(K) = PermBuf(j)
            PermBuf(j) = TmpI
            j = PermBuf(K)
            For i_ = 0# To NVars Step 1
                XYS(K, i_) = XY(j, i_)
            Next i_
        Next K
        
        '
        ' build tree, copy
        '
        Call DFBuildTree(XYS, SampleSize, NVars, NClasses, NFeatures, NVarsInPool, Flags, Bufs)
        j = Round(Bufs.TreeBuf(0#))
        i1_ = (0#) - (Offs)
        For i_ = Offs To Offs + j - 1# Step 1
            DF.Trees(i_) = Bufs.TreeBuf(i_ + i1_)
        Next i_
        LastTreeOffs = Offs
        Offs = Offs + j
        
        '
        ' OOB estimates
        '
        For K = SampleSize To NPoints - 1# Step 1
            For j = 0# To NClasses - 1# Step 1
                y(j) = 0#
            Next j
            j = PermBuf(K)
            For i_ = 0# To NVars - 1# Step 1
                X(i_) = XY(j, i_)
            Next i_
            Call DFProcessInternal(DF, LastTreeOffs, X, y)
            i1_ = (0#) - (j * NClasses)
            For i_ = j * NClasses To (j + 1#) * NClasses - 1# Step 1
                OOBBuf(i_) = OOBBuf(i_) + y(i_ + i1_)
            Next i_
            OOBCntBuf(j) = OOBCntBuf(j) + 1#
        Next K
    Next i
    DF.BufSize = Offs
    
    '
    ' Normalize OOB results
    '
    For i = 0# To NPoints - 1# Step 1
        If OOBCntBuf(i) <> 0# Then
            V = 1# / OOBCntBuf(i)
            For i_ = i * NClasses To i * NClasses + NClasses - 1# Step 1
                OOBBuf(i_) = V * OOBBuf(i_)
            Next i_
        End If
    Next i
    
    '
    ' Calculate training set estimates
    '
    Rep.RelCLSError = DFRelClsError(DF, XY, NPoints)
    Rep.AvgCE = DFAvgCE(DF, XY, NPoints)
    Rep.RMSError = DFRMSError(DF, XY, NPoints)
    Rep.AvgError = DFAvgError(DF, XY, NPoints)
    Rep.AvgRelError = DFAvgRelError(DF, XY, NPoints)
    
    '
    ' Calculate OOB estimates.
    '
    Rep.OOBRelClsError = 0#
    Rep.OOBAvgCE = 0#
    Rep.OOBRMSError = 0#
    Rep.OOBAvgError = 0#
    Rep.OOBAvgRelError = 0#
    OOBCnt = 0#
    OOBRelCnt = 0#
    For i = 0# To NPoints - 1# Step 1
        If OOBCntBuf(i) <> 0# Then
            OOBOffs = i * NClasses
            If NClasses > 1# Then
                
                '
                ' classification-specific code
                '
                K = Round(XY(i, NVars))
                TmpI = 0#
                For j = 1# To NClasses - 1# Step 1
                    If OOBBuf(OOBOffs + j) > OOBBuf(OOBOffs + TmpI) Then
                        TmpI = j
                    End If
                Next j
                If TmpI <> K Then
                    Rep.OOBRelClsError = Rep.OOBRelClsError + 1#
                End If
                If OOBBuf(OOBOffs + K) <> 0# Then
                    Rep.OOBAvgCE = Rep.OOBAvgCE - Log(OOBBuf(OOBOffs + K))
                Else
                    Rep.OOBAvgCE = Rep.OOBAvgCE - Log(MinRealNumber)
                End If
                For j = 0# To NClasses - 1# Step 1
                    If j = K Then
                        Rep.OOBRMSError = Rep.OOBRMSError + Square(OOBBuf(OOBOffs + j) - 1#)
                        Rep.OOBAvgError = Rep.OOBAvgError + Abs(OOBBuf(OOBOffs + j) - 1#)
                        Rep.OOBAvgRelError = Rep.OOBAvgRelError + Abs(OOBBuf(OOBOffs + j) - 1#)
                        OOBRelCnt = OOBRelCnt + 1#
                    Else
                        Rep.OOBRMSError = Rep.OOBRMSError + Square(OOBBuf(OOBOffs + j))
                        Rep.OOBAvgError = Rep.OOBAvgError + Abs(OOBBuf(OOBOffs + j))
                    End If
                Next j
            Else
                
                '
                ' regression-specific code
                '
                Rep.OOBRMSError = Rep.OOBRMSError + Square(OOBBuf(OOBOffs) - XY(i, NVars))
                Rep.OOBAvgError = Rep.OOBAvgError + Abs(OOBBuf(OOBOffs) - XY(i, NVars))
                If XY(i, NVars) <> 0# Then
                    Rep.OOBAvgRelError = Rep.OOBAvgRelError + Abs((OOBBuf(OOBOffs) - XY(i, NVars)) / XY(i, NVars))
                    OOBRelCnt = OOBRelCnt + 1#
                End If
            End If
            
            '
            ' update OOB estimates count.
            '
            OOBCnt = OOBCnt + 1#
        End If
    Next i
    If OOBCnt > 0# Then
        Rep.OOBRelClsError = Rep.OOBRelClsError / OOBCnt
        Rep.OOBAvgCE = Rep.OOBAvgCE / OOBCnt
        Rep.OOBRMSError = Sqr(Rep.OOBRMSError / (OOBCnt * NClasses))
        Rep.OOBAvgError = Rep.OOBAvgError / (OOBCnt * NClasses)
        If OOBRelCnt > 0# Then
            Rep.OOBAvgRelError = Rep.OOBAvgRelError / OOBRelCnt
        End If
    End If
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Procesing
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    X       -   input vector,  array[0..NVars-1].
'
'OUTPUT PARAMETERS:
'    Y       -   result. Regression estimate when solving regression  task,
'                vector of posterior probabilities for classification task.
'                Subroutine does not allocate memory for this vector, it is
'                responsibility of a caller to allocate it. Array  must  be
'                at least [0..NClasses-1].
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Sub DFProcess(ByRef DF As DecisionForest, _
         ByRef X() As Double, _
         ByRef y() As Double)
    Dim Offs As Long
    Dim i As Long
    Dim V As Double
    Dim i_ As Long
    
    '
    ' Proceed
    '
    Offs = 0#
    For i = 0# To DF.NClasses - 1# Step 1
        y(i) = 0#
    Next i
    For i = 0# To DF.NTrees - 1# Step 1
        
        '
        ' Process basic tree
        '
        Call DFProcessInternal(DF, Offs, X, y)
        
        '
        ' Next tree
        '
        Offs = Offs + Round(DF.Trees(Offs))
    Next i
    V = 1# / DF.NTrees
    For i_ = 0# To DF.NClasses - 1# Step 1
        y(i_) = V * y(i_)
    Next i_
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Relative classification error on the test set
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    XY      -   test set
'    NPoints -   test set size
'
'RESULT:
'    percent of incorrectly classified cases.
'    Zero if model solves regression task.
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Function DFRelClsError(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Double
    Dim Result As Double
    Result = DFClsError(DF, XY, NPoints) / NPoints
    DFRelClsError = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Average cross-entropy (in bits per element) on the test set
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    XY      -   test set
'    NPoints -   test set size
'
'RESULT:
'    CrossEntropy/(NPoints*LN(2)).
'    Zero if model solves regression task.
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Function DFAvgCE(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Double
    Dim Result As Double
    Dim X() As Double
    Dim y() As Double
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim TmpI As Long
    Dim i_ As Long
    ReDim X(0# To DF.NVars - 1#)
    ReDim y(0# To DF.NClasses - 1#)
    Result = 0#
    For i = 0# To NPoints - 1# Step 1
        For i_ = 0# To DF.NVars - 1# Step 1
            X(i_) = XY(i, i_)
        Next i_
        Call DFProcess(DF, X, y)
        If DF.NClasses > 1# Then
            
            '
            ' classification-specific code
            '
            K = Round(XY(i, DF.NVars))
            TmpI = 0#
            For j = 1# To DF.NClasses - 1# Step 1
                If y(j) > y(TmpI) Then
                    TmpI = j
                End If
            Next j
            If y(K) <> 0# Then
                Result = Result - Log(y(K))
            Else
                Result = Result - Log(MinRealNumber)
            End If
        End If
    Next i
    Result = Result / NPoints
    DFAvgCE = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'RMS error on the test set
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    XY      -   test set
'    NPoints -   test set size
'
'RESULT:
'    root mean square error.
'    Its meaning for regression task is obvious. As for
'    classification task, RMS error means error when estimating posterior
'    probabilities.
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Function DFRMSError(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Double
    Dim Result As Double
    Dim X() As Double
    Dim y() As Double
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim TmpI As Long
    Dim i_ As Long
    ReDim X(0# To DF.NVars - 1#)
    ReDim y(0# To DF.NClasses - 1#)
    Result = 0#
    For i = 0# To NPoints - 1# Step 1
        For i_ = 0# To DF.NVars - 1# Step 1
            X(i_) = XY(i, i_)
        Next i_
        Call DFProcess(DF, X, y)
        If DF.NClasses > 1# Then
            
            '
            ' classification-specific code
            '
            K = Round(XY(i, DF.NVars))
            TmpI = 0#
            For j = 1# To DF.NClasses - 1# Step 1
                If y(j) > y(TmpI) Then
                    TmpI = j
                End If
            Next j
            For j = 0# To DF.NClasses - 1# Step 1
                If j = K Then
                    Result = Result + Square(y(j) - 1#)
                Else
                    Result = Result + Square(y(j))
                End If
            Next j
        Else
            
            '
            ' regression-specific code
            '
            Result = Result + Square(y(0#) - XY(i, DF.NVars))
        End If
    Next i
    Result = Sqr(Result / (NPoints * DF.NClasses))
    DFRMSError = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Average error on the test set
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    XY      -   test set
'    NPoints -   test set size
'
'RESULT:
'    Its meaning for regression task is obvious. As for
'    classification task, it means average error when estimating posterior
'    probabilities.
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Function DFAvgError(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Double
    Dim Result As Double
    Dim X() As Double
    Dim y() As Double
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim i_ As Long
    ReDim X(0# To DF.NVars - 1#)
    ReDim y(0# To DF.NClasses - 1#)
    Result = 0#
    For i = 0# To NPoints - 1# Step 1
        For i_ = 0# To DF.NVars - 1# Step 1
            X(i_) = XY(i, i_)
        Next i_
        Call DFProcess(DF, X, y)
        If DF.NClasses > 1# Then
            
            '
            ' classification-specific code
            '
            K = Round(XY(i, DF.NVars))
            For j = 0# To DF.NClasses - 1# Step 1
                If j = K Then
                    Result = Result + Abs(y(j) - 1#)
                Else
                    Result = Result + Abs(y(j))
                End If
            Next j
        Else
            
            '
            ' regression-specific code
            '
            Result = Result + Abs(y(0#) - XY(i, DF.NVars))
        End If
    Next i
    Result = Result / (NPoints * DF.NClasses)
    DFAvgError = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Average relative error on the test set
'
'INPUT PARAMETERS:
'    DF      -   decision forest model
'    XY      -   test set
'    NPoints -   test set size
'
'RESULT:
'    Its meaning for regression task is obvious. As for
'    classification task, it means average relative error when estimating
'    posterior probability of belonging to the correct class.
'
'  -- ALGLIB --
'     Copyright 16.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Function DFAvgRelError(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Double
    Dim Result As Double
    Dim X() As Double
    Dim y() As Double
    Dim RelCnt As Long
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim i_ As Long
    ReDim X(0# To DF.NVars - 1#)
    ReDim y(0# To DF.NClasses - 1#)
    Result = 0#
    RelCnt = 0#
    For i = 0# To NPoints - 1# Step 1
        For i_ = 0# To DF.NVars - 1# Step 1
            X(i_) = XY(i, i_)
        Next i_
        Call DFProcess(DF, X, y)
        If DF.NClasses > 1# Then
            
            '
            ' classification-specific code
            '
            K = Round(XY(i, DF.NVars))
            For j = 0# To DF.NClasses - 1# Step 1
                If j = K Then
                    Result = Result + Abs(y(j) - 1#)
                    RelCnt = RelCnt + 1#
                End If
            Next j
        Else
            
            '
            ' regression-specific code
            '
            If XY(i, DF.NVars) <> 0# Then
                Result = Result + Abs((y(0#) - XY(i, DF.NVars)) / XY(i, DF.NVars))
                RelCnt = RelCnt + 1#
            End If
        End If
    Next i
    If RelCnt > 0# Then
        Result = Result / RelCnt
    End If
    DFAvgRelError = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Copying of DecisionForest strucure
'
'INPUT PARAMETERS:
'    DF1 -   original
'
'OUTPUT PARAMETERS:
'    DF2 -   copy
'
'  -- ALGLIB --
'     Copyright 13.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Sub DFCopy(ByRef DF1 As DecisionForest, ByRef DF2 As DecisionForest)
    Dim i_ As Long
    DF2.NVars = DF1.NVars
    DF2.NClasses = DF1.NClasses
    DF2.NTrees = DF1.NTrees
    DF2.BufSize = DF1.BufSize
    ReDim DF2.Trees(0# To DF1.BufSize - 1#)
    For i_ = 0# To DF1.BufSize - 1# Step 1
        DF2.Trees(i_) = DF1.Trees(i_)
    Next i_
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Serialization of DecisionForest strucure
'
'INPUT PARAMETERS:
'    DF      -   original
'
'OUTPUT PARAMETERS:
'    RA      -   array of real numbers which stores decision forest,
'                array[0..RLen-1]
'    RLen    -   RA lenght
'
'  -- ALGLIB --
'     Copyright 13.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Sub DFSerialize(ByRef DF As DecisionForest, _
         ByRef RA() As Double, _
         ByRef RLen As Long)
    Dim i_ As Long
    Dim i1_ As Long
    ReDim RA(0# To DF.BufSize + 5# - 1#)
    RA(0#) = DFVNum
    RA(1#) = DF.NVars
    RA(2#) = DF.NClasses
    RA(3#) = DF.NTrees
    RA(4#) = DF.BufSize
    i1_ = (0#) - (5#)
    For i_ = 5# To 5# + DF.BufSize - 1# Step 1
        RA(i_) = DF.Trees(i_ + i1_)
    Next i_
    RLen = 5# + DF.BufSize
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Unserialization of DecisionForest strucure
'
'INPUT PARAMETERS:
'    RA      -   real array which stores decision forest
'
'OUTPUT PARAMETERS:
'    DF      -   restored structure
'
'  -- ALGLIB --
'     Copyright 13.02.2009 by Bochkanov Sergey
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Public Sub DFUnserialize(ByRef RA() As Double, ByRef DF As DecisionForest)
    Dim i_ As Long
    Dim i1_ As Long
    DF.NVars = Round(RA(1#))
    DF.NClasses = Round(RA(2#))
    DF.NTrees = Round(RA(3#))
    DF.BufSize = Round(RA(4#))
    ReDim DF.Trees(0# To DF.BufSize - 1#)
    i1_ = (5#) - (0#)
    For i_ = 0# To DF.BufSize - 1# Step 1
        DF.Trees(i_) = RA(i_ + i1_)
    Next i_
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Classification error
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Function DFClsError(ByRef DF As DecisionForest, _
         ByRef XY() As Double, _
         ByVal NPoints As Long) As Long
    Dim Result As Long
    Dim X() As Double
    Dim y() As Double
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim TmpI As Long
    Dim i_ As Long
    If DF.NClasses <= 1# Then
        Result = 0#
        DFClsError = Result
        Exit Function
    End If
    ReDim X(0# To DF.NVars - 1#)
    ReDim y(0# To DF.NClasses - 1#)
    Result = 0#
    For i = 0# To NPoints - 1# Step 1
        For i_ = 0# To DF.NVars - 1# Step 1
            X(i_) = XY(i, i_)
        Next i_
        Call DFProcess(DF, X, y)
        K = Round(XY(i, DF.NVars))
        TmpI = 0#
        For j = 1# To DF.NClasses - 1# Step 1
            If y(j) > y(TmpI) Then
                TmpI = j
            End If
        Next j
        If TmpI <> K Then
            Result = Result + 1#
        End If
    Next i
    DFClsError = Result
End Function
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Internal subroutine for processing one decision tree starting at Offs
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFProcessInternal(ByRef DF As DecisionForest, _
         ByVal Offs As Long, _
         ByRef X() As Double, _
         ByRef y() As Double)
    Dim K As Long
    Dim Idx As Long
    
    '
    ' Set pointer to the root
    '
    K = Offs + 1#
    
    '
    ' Navigate through the tree
    '
    Do While True
        If DF.Trees(K) = -1# Then
            If DF.NClasses = 1# Then
                y(0#) = y(0#) + DF.Trees(K + 1#)
            Else
                Idx = Round(DF.Trees(K + 1#))
                y(Idx) = y(Idx) + 1#
            End If
            Exit Do
        End If
        If X(Round(DF.Trees(K))) < DF.Trees(K + 1#) Then
            K = K + InnerNodeWidth
        Else
            K = Offs + Round(DF.Trees(K + 2#))
        End If
    Loop
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Builds one decision tree. Just a wrapper for the DFBuildTreeRec.
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFBuildTree(ByRef XY() As Double, _
         ByVal NPoints As Long, _
         ByVal NVars As Long, _
         ByVal NClasses As Long, _
         ByVal NFeatures As Long, _
         ByVal NVarsInPool As Long, _
         ByVal Flags As Long, _
         ByRef Bufs As DFInternalBuffers)
    Dim NumProcessed As Long
    Dim i As Long
    
    '
    ' Prepare IdxBuf. It stores indices of the training set elements.
    ' When training set is being split, contents of IdxBuf is
    ' correspondingly reordered so we can know which elements belong
    ' to which branch of decision tree.
    '
    For i = 0# To NPoints - 1# Step 1
        Bufs.IdxBuf(i) = i
    Next i
    
    '
    ' Recursive procedure
    '
    NumProcessed = 1#
    Call DFBuildTreeRec(XY, NPoints, NVars, NClasses, NFeatures, NVarsInPool, Flags, NumProcessed, 0#, NPoints - 1#, Bufs)
    Bufs.TreeBuf(0#) = NumProcessed
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Builds one decision tree (internal recursive subroutine)
'
'Parameters:
'    TreeBuf     -   large enough array, at least TreeSize
'    IdxBuf      -   at least NPoints elements
'    TmpBufR     -   at least NPoints
'    TmpBufR2    -   at least NPoints
'    TmpBufI     -   at least NPoints
'    TmpBufI2    -   at least NPoints+1
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFBuildTreeRec(ByRef XY() As Double, _
         ByVal NPoints As Long, _
         ByVal NVars As Long, _
         ByVal NClasses As Long, _
         ByVal NFeatures As Long, _
         ByVal NVarsInPool As Long, _
         ByVal Flags As Long, _
         ByRef NumProcessed As Long, _
         ByVal Idx1 As Long, _
         ByVal Idx2 As Long, _
         ByRef Bufs As DFInternalBuffers)
    Dim i As Long
    Dim j As Long
    Dim K As Long
    Dim BFlag As Boolean
    Dim I1 As Long
    Dim I2 As Long
    Dim Info As Long
    Dim SL As Double
    Dim SR As Double
    Dim w As Double
    Dim IdxBest As Long
    Dim EBest As Double
    Dim TBest As Double
    Dim VarCur As Long
    Dim S As Double
    Dim V As Double
    Dim V1 As Double
    Dim V2 As Double
    Dim Threshold As Double
    Dim OldNP As Long
    Dim CurRMS As Double
    Dim UseEVS As Boolean
    UseEVS = Flags \ DFUseEVS Mod 2# <> 0#
    
    '
    ' Leaf node
    '
    If Idx2 = Idx1 Then
        Bufs.TreeBuf(NumProcessed) = -1#
        Bufs.TreeBuf(NumProcessed + 1#) = XY(Bufs.IdxBuf(Idx1), NVars)
        NumProcessed = NumProcessed + LeafNodeWidth
        Exit Sub
    End If
    
    '
    ' Non-leaf node.
    ' Select random variable, prepare split:
    ' 1. prepare default solution - no splitting, class at random
    ' 2. investigate possible splits, compare with default/best
    '
    IdxBest = -1#
    If NClasses > 1# Then
        
        '
        ' default solution for classification
        '
        For i = 0# To NClasses - 1# Step 1
            Bufs.ClassIBuf(i) = 0#
        Next i
        S = Idx2 - Idx1 + 1#
        For i = Idx1 To Idx2 Step 1
            j = Round(XY(Bufs.IdxBuf(i), NVars))
            Bufs.ClassIBuf(j) = Bufs.ClassIBuf(j) + 1#
        Next i
        EBest = 0#
        For i = 0# To NClasses - 1# Step 1
            EBest = EBest + Bufs.ClassIBuf(i) * Square(1# - Bufs.ClassIBuf(i) / S) + (S - Bufs.ClassIBuf(i)) * Square(Bufs.ClassIBuf(i) / S)
        Next i
        EBest = Sqr(EBest / (NClasses * (Idx2 - Idx1 + 1#)))
    Else
        
        '
        ' default solution for regression
        '
        V = 0#
        For i = Idx1 To Idx2 Step 1
            V = V + XY(Bufs.IdxBuf(i), NVars)
        Next i
        V = V / (Idx2 - Idx1 + 1#)
        EBest = 0#
        For i = Idx1 To Idx2 Step 1
            EBest = EBest + Square(XY(Bufs.IdxBuf(i), NVars) - V)
        Next i
        EBest = Sqr(EBest / (Idx2 - Idx1 + 1#))
    End If
    i = 0#
    Do While i <= MinInt(NFeatures, NVarsInPool) - 1#
        
        '
        ' select variables from pool
        '
        j = i + RandomInteger(NVarsInPool - i)
        K = Bufs.VarPool(i)
        Bufs.VarPool(i) = Bufs.VarPool(j)
        Bufs.VarPool(j) = K
        VarCur = Bufs.VarPool(i)
        
        '
        ' load variable values to working array
        '
        ' apply EVS preprocessing: if all variable values are same,
        ' variable is excluded from pool.
        '
        ' This is necessary for binary pre-splits (see later) to work.
        '
        For j = Idx1 To Idx2 Step 1
            Bufs.TmpBufR(j - Idx1) = XY(Bufs.IdxBuf(j), VarCur)
        Next j
        If UseEVS Then
            BFlag = False
            V = Bufs.TmpBufR(0#)
            For j = 0# To Idx2 - Idx1 Step 1
                If Bufs.TmpBufR(j) <> V Then
                    BFlag = True
                    Exit For
                End If
            Next j
            If Not BFlag Then
                
                '
                ' exclude variable from pool,
                ' go to the next iteration.
                ' I is not increased.
                '
                K = Bufs.VarPool(i)
                Bufs.VarPool(i) = Bufs.VarPool(NVarsInPool - 1#)
                Bufs.VarPool(NVarsInPool - 1#) = K
                NVarsInPool = NVarsInPool - 1#
                GoTo Cont_6
            End If
        End If
        
        '
        ' load labels to working array
        '
        If NClasses > 1# Then
            For j = Idx1 To Idx2 Step 1
                Bufs.TmpBufI(j - Idx1) = Round(XY(Bufs.IdxBuf(j), NVars))
            Next j
        Else
            For j = Idx1 To Idx2 Step 1
                Bufs.TmpBufR2(j - Idx1) = XY(Bufs.IdxBuf(j), NVars)
            Next j
        End If
        
        '
        ' calculate split
        '
        If UseEVS And Bufs.EVSBin(VarCur) Then
            
            '
            ' Pre-calculated splits for binary variables.
            ' Threshold is already known, just calculate RMS error
            '
            Threshold = Bufs.EVSSplits(VarCur)
            If NClasses > 1# Then
                
                '
                ' classification-specific code
                '
                For j = 0# To 2# * NClasses - 1# Step 1
                    Bufs.ClassIBuf(j) = 0#
                Next j
                SL = 0#
                SR = 0#
                For j = 0# To Idx2 - Idx1 Step 1
                    K = Bufs.TmpBufI(j)
                    If Bufs.TmpBufR(j) < Threshold Then
                        Bufs.ClassIBuf(K) = Bufs.ClassIBuf(K) + 1#
                        SL = SL + 1#
                    Else
                        Bufs.ClassIBuf(K + NClasses) = Bufs.ClassIBuf(K + NClasses) + 1#
                        SR = SR + 1#
                    End If
                Next j
                CurRMS = 0#
                For j = 0# To NClasses - 1# Step 1
                    w = Bufs.ClassIBuf(j)
                    CurRMS = CurRMS + w * Square(w / SL - 1#)
                    CurRMS = CurRMS + (SL - w) * Square(w / SL)
                    w = Bufs.ClassIBuf(NClasses + j)
                    CurRMS = CurRMS + w * Square(w / SR - 1#)
                    CurRMS = CurRMS + (SR - w) * Square(w / SR)
                Next j
                CurRMS = Sqr(CurRMS / (NClasses * (Idx2 - Idx1 + 1#)))
            Else
                
                '
                ' regression-specific code
                '
                SL = 0#
                SR = 0#
                V1 = 0#
                V2 = 0#
                For j = 0# To Idx2 - Idx1 Step 1
                    If Bufs.TmpBufR(j) < Threshold Then
                        V1 = V1 + Bufs.TmpBufR2(j)
                        SL = SL + 1#
                    Else
                        V2 = V2 + Bufs.TmpBufR2(j)
                        SR = SR + 1#
                    End If
                Next j
                V1 = V1 / SL
                V2 = V2 / SR
                CurRMS = 0#
                For j = 0# To Idx2 - Idx1 Step 1
                    If Bufs.TmpBufR(j) < Threshold Then
                        CurRMS = CurRMS + Square(V1 - Bufs.TmpBufR2(j))
                    Else
                        CurRMS = CurRMS + Square(V2 - Bufs.TmpBufR2(j))
                    End If
                Next j
                CurRMS = Sqr(CurRMS / (Idx2 - Idx1 + 1#))
            End If
            Info = 1#
        Else
            
            '
            ' Generic splits
            '
            If NClasses > 1# Then
                Call DFSplitC(Bufs.TmpBufR, Bufs.TmpBufI, Bufs.ClassIBuf, Idx2 - Idx1 + 1#, NClasses, DFUseStrongSplits, Info, Threshold, CurRMS)
            Else
                Call DFSplitR(Bufs.TmpBufR, Bufs.TmpBufR2, Idx2 - Idx1 + 1#, DFUseStrongSplits, Info, Threshold, CurRMS)
            End If
        End If
        If Info > 0# Then
            If CurRMS <= EBest Then
                EBest = CurRMS
                IdxBest = VarCur
                TBest = Threshold
            End If
        End If
        
        '
        ' Next iteration
        '
        i = i + 1#
Cont_6:
    Loop
    
    '
    ' to split or not to split
    '
    If IdxBest < 0# Then
        
        '
        ' All values are same, cannot split.
        '
        Bufs.TreeBuf(NumProcessed) = -1#
        If NClasses > 1# Then
            
            '
            ' Select random class label (randomness allows us to
            ' approximate distribution of the classes)
            '
            Bufs.TreeBuf(NumProcessed + 1#) = Round(XY(Bufs.IdxBuf(Idx1 + RandomInteger(Idx2 - Idx1 + 1#)), NVars))
        Else
            
            '
            ' Select average (for regression task).
            '
            V = 0#
            For i = Idx1 To Idx2 Step 1
                V = V + XY(Bufs.IdxBuf(i), NVars) / (Idx2 - Idx1 + 1#)
            Next i
            Bufs.TreeBuf(NumProcessed + 1#) = V
        End If
        NumProcessed = NumProcessed + LeafNodeWidth
    Else
        
        '
        ' we can split
        '
        Bufs.TreeBuf(NumProcessed) = IdxBest
        Bufs.TreeBuf(NumProcessed + 1#) = TBest
        I1 = Idx1
        I2 = Idx2
        Do While I1 <= I2
            
            '
            ' Reorder indices so that left partition is in [Idx1..I1-1],
            ' and right partition is in [I2+1..Idx2]
            '
            If XY(Bufs.IdxBuf(I1), IdxBest) < TBest Then
                I1 = I1 + 1#
                GoTo Cont_17
            End If
            If XY(Bufs.IdxBuf(I2), IdxBest) >= TBest Then
                I2 = I2 - 1#
                GoTo Cont_17
            End If
            j = Bufs.IdxBuf(I1)
            Bufs.IdxBuf(I1) = Bufs.IdxBuf(I2)
            Bufs.IdxBuf(I2) = j
            I1 = I1 + 1#
            I2 = I2 - 1#
Cont_17:
        Loop
        OldNP = NumProcessed
        NumProcessed = NumProcessed + InnerNodeWidth
        Call DFBuildTreeRec(XY, NPoints, NVars, NClasses, NFeatures, NVarsInPool, Flags, NumProcessed, Idx1, I1 - 1#, Bufs)
        Bufs.TreeBuf(OldNP + 2#) = NumProcessed
        Call DFBuildTreeRec(XY, NPoints, NVars, NClasses, NFeatures, NVarsInPool, Flags, NumProcessed, I2 + 1#, Idx2, Bufs)
    End If
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Makes weak split on attribute
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFWeakSplitI(ByRef X() As Double, _
         ByRef y() As Long, _
         ByVal N As Long, _
         ByVal NClasses As Long, _
         ByRef Info As Long, _
         ByRef Threshold As Double, _
         ByRef E As Double)
    Dim i As Long
    Dim NEq As Long
    Dim NLess As Long
    Dim NGreater As Long
    Call TagSortFastI(X, y, N)
    If N Mod 2# = 1# Then
        
        '
        ' odd number of elements
        '
        Threshold = X(N \ 2#)
    Else
        
        '
        ' even number of elements.
        '
        ' if two closest to the middle of the array are equal,
        ' we will select one of them (to avoid possible problems with
        ' floating point errors).
        ' we will select halfsum otherwise.
        '
        If X(N \ 2# - 1#) = X(N \ 2#) Then
            Threshold = X(N \ 2# - 1#)
        Else
            Threshold = 0.5 * (X(N \ 2# - 1#) + X(N \ 2#))
        End If
    End If
    NEq = 0#
    NLess = 0#
    NGreater = 0#
    For i = 0# To N - 1# Step 1
        If X(i) < Threshold Then
            NLess = NLess + 1#
        End If
        If X(i) = Threshold Then
            NEq = NEq + 1#
        End If
        If X(i) > Threshold Then
            NGreater = NGreater + 1#
        End If
    Next i
    If NLess = 0# And NGreater = 0# Then
        Info = -3#
    Else
        If NEq <> 0# Then
            If NLess < NGreater Then
                Threshold = 0.5 * (X(NLess + NEq - 1#) + X(NLess + NEq))
            Else
                Threshold = 0.5 * (X(NLess - 1#) + X(NLess))
            End If
        End If
        Info = 1#
        E = 0#
    End If
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Makes split on attribute
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFSplitC(ByRef X() As Double, _
         ByRef C() As Long, _
         ByRef CntBuf() As Long, _
         ByVal N As Long, _
         ByVal NC As Long, _
         ByVal Flags As Long, _
         ByRef Info As Long, _
         ByRef Threshold As Double, _
         ByRef E As Double)
    Dim i As Long
    Dim NEq As Long
    Dim NLess As Long
    Dim NGreater As Long
    Dim q As Long
    Dim QMin As Long
    Dim QMax As Long
    Dim QCnt As Long
    Dim CurSplit As Double
    Dim NLeft As Long
    Dim V As Double
    Dim CurE As Double
    Dim w As Double
    Dim SL As Double
    Dim SR As Double
    Call TagSortFastI(X, C, N)
    E = MaxRealNumber
    Threshold = 0.5 * (X(0#) + X(N - 1#))
    Info = -3#
    If Flags \ DFUseStrongSplits Mod 2# = 0# Then
        
        '
        ' weak splits, split at half
        '
        QCnt = 2#
        QMin = 1#
        QMax = 1#
    Else
        
        '
        ' strong splits: choose best quartile
        '
        QCnt = 4#
        QMin = 1#
        QMax = 3#
    End If
    For q = QMin To QMax Step 1
        CurSplit = X(N * q \ QCnt)
        NEq = 0#
        NLess = 0#
        NGreater = 0#
        For i = 0# To N - 1# Step 1
            If X(i) < CurSplit Then
                NLess = NLess + 1#
            End If
            If X(i) = CurSplit Then
                NEq = NEq + 1#
            End If
            If X(i) > CurSplit Then
                NGreater = NGreater + 1#
            End If
        Next i
        If NLess <> 0# Or NGreater <> 0# Then
            
            '
            ' set threshold between two partitions, with
            ' some tweaking to avoid problems with floating point
            ' arithmetics.
            '
            ' The problem is that when you calculates C = 0.5*(A+B) there
            ' can be no C which lies strictly between A and B (for example,
            ' there is no floating point number which is
            ' greater than 1 and less than 1+eps). In such situations
            ' we choose right side as theshold (remember that
            ' points which lie on threshold falls to the right side).
            '
            If NLess < NGreater Then
                CurSplit = 0.5 * (X(NLess + NEq - 1#) + X(NLess + NEq))
                NLeft = NLess + NEq
                If CurSplit <= X(NLess + NEq - 1#) Then
                    CurSplit = X(NLess + NEq)
                End If
            Else
                CurSplit = 0.5 * (X(NLess - 1#) + X(NLess))
                NLeft = NLess
                If CurSplit <= X(NLess - 1#) Then
                    CurSplit = X(NLess)
                End If
            End If
            Info = 1#
            CurE = 0#
            For i = 0# To 2# * NC - 1# Step 1
                CntBuf(i) = 0#
            Next i
            For i = 0# To NLeft - 1# Step 1
                CntBuf(C(i)) = CntBuf(C(i)) + 1#
            Next i
            For i = NLeft To N - 1# Step 1
                CntBuf(NC + C(i)) = CntBuf(NC + C(i)) + 1#
            Next i
            SL = NLeft
            SR = N - NLeft
            V = 0#
            For i = 0# To NC - 1# Step 1
                w = CntBuf(i)
                V = V + w * Square(w / SL - 1#)
                V = V + (SL - w) * Square(w / SL)
                w = CntBuf(NC + i)
                V = V + w * Square(w / SR - 1#)
                V = V + (SR - w) * Square(w / SR)
            Next i
            CurE = Sqr(V / (NC * N))
            If CurE < E Then
                Threshold = CurSplit
                E = CurE
            End If
        End If
    Next q
End Sub
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
'Makes split on attribute
'
''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''
Private Sub DFSplitR(ByRef X() As Double, _
         ByRef y() As Double, _
         ByVal N As Long, _
         ByVal Flags As Long, _
         ByRef Info As Long, _
         ByRef Threshold As Double, _
         ByRef E As Double)
    Dim i As Long
    Dim NEq As Long
    Dim NLess As Long
    Dim NGreater As Long
    Dim q As Long
    Dim QMin As Long
    Dim QMax As Long
    Dim QCnt As Long
    Dim CurSplit As Double
    Dim NLeft As Long
    Dim V As Double
    Dim CurE As Double
    Call TagSortFastR(X, y, N)
    E = MaxRealNumber
    Threshold = 0.5 * (X(0#) + X(N - 1#))
    Info = -3#
    If Flags \ DFUseStrongSplits Mod 2# = 0# Then
        
        '
        ' weak splits, split at half
        '
        QCnt = 2#
        QMin = 1#
        QMax = 1#
    Else
        
        '
        ' strong splits: choose best quartile
        '
        QCnt = 4#
        QMin = 1#
        QMax = 3#
    End If
    For q = QMin To QMax Step 1
        CurSplit = X(N * q \ QCnt)
        NEq = 0#
        NLess = 0#
        NGreater = 0#
        For i = 0# To N - 1# Step 1
            If X(i) < CurSplit Then
                NLess = NLess + 1#
            End If
            If X(i) = CurSplit Then
                NEq = NEq + 1#
            End If
            If X(i) > CurSplit Then
                NGreater = NGreater + 1#
            End If
        Next i
        If NLess <> 0# Or NGreater <> 0# Then
            
            '
            ' set threshold between two partitions, with
            ' some tweaking to avoid problems with floating point
            ' arithmetics.
            '
            ' The problem is that when you calculates C = 0.5*(A+B) there
            ' can be no C which lies strictly between A and B (for example,
            ' there is no floating point number which is
            ' greater than 1 and less than 1+eps). In such situations
            ' we choose right side as theshold (remember that
            ' points which lie on threshold falls to the right side).
            '
            If NLess < NGreater Then
                CurSplit = 0.5 * (X(NLess + NEq - 1#) + X(NLess + NEq))
                NLeft = NLess + NEq
                If CurSplit <= X(NLess + NEq - 1#) Then
                    CurSplit = X(NLess + NEq)
                End If
            Else
                CurSplit = 0.5 * (X(NLess - 1#) + X(NLess))
                NLeft = NLess
                If CurSplit <= X(NLess - 1#) Then
                    CurSplit = X(NLess)
                End If
            End If
            Info = 1#
            CurE = 0#
            V = 0#
            For i = 0# To NLeft - 1# Step 1
                V = V + y(i)
            Next i
            V = V / NLeft
            For i = 0# To NLeft - 1# Step 1
                CurE = CurE + Square(y(i) - V)
            Next i
            V = 0#
            For i = NLeft To N - 1# Step 1
                V = V + y(i)
            Next i
            V = V / (N - NLeft)
            For i = NLeft To N - 1# Step 1
                CurE = CurE + Square(y(i) - V)
            Next i
            CurE = Sqr(CurE / N)
            If CurE < E Then
                Threshold = CurSplit
                E = CurE
            End If
        End If
    Next q
End Sub

